import os
import json
import traceback
import google.generativeai as genai
from dotenv import load_dotenv
import time

# services klasÃ¶rÃ¼nden import
from services import calculator, search

# Ortam deÄŸiÅŸkenlerini yÃ¼kle (.env)
load_dotenv()

# Gemini client baÅŸlat
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))


class WebChatbot:
    def __init__(self):
        # BaÅŸlangÄ±Ã§ sistem mesajÄ± (asistanÄ±n rolÃ¼nÃ¼ tanÄ±mlar)
        self.system_prompt = """Sen Wikipedia entegrasyonlu uzman bir asistansÄ±n. ChatGPT gibi net, anlaÅŸÄ±lÄ±r ve doÄŸrudan cevaplar ver.

**CEVAP FORMATI KURALLARI:**

ðŸ“š **Vikipedi Entegrasyonu:**
- Wikipedia bilgilerini Ã¶zetle ve dÃ¼zenle
- KarmaÅŸÄ±k bilgileri basitleÅŸtir
- Kaynak gÃ¶stermek iÃ§in [1], [2] gibi referanslar kullan
- Bilgileri gÃ¼ncel ve doÄŸru tut

ðŸŽ¯ **YapÄ±landÄ±rma:**
- Konuyu mantÄ±klÄ± bÃ¶lÃ¼mlere ayÄ±r
- Ana baÅŸlÄ±klar iÃ§in ##, alt baÅŸlÄ±klar iÃ§in ### kullan
- Ã–nemli tarihleri ve isimleri **kalÄ±n** ile vurgula
- Listeler iÃ§in â€¢ kullan
- Kronolojik sÄ±raya dikkat et

ðŸ’¬ **KonuÅŸma TarzÄ±:**
- Bilgilendirici ama sÄ±kmayan
- Akademik dilden kaÃ§Ä±n, anlaÅŸÄ±lÄ±r ol
- Gereksiz detaylarla boÄŸma
- Ã–nemli noktalarÄ± Ã¶ne Ã§Ä±kar

ðŸ” **AraÅŸtÄ±rma YaklaÅŸÄ±mÄ±:**
- KullanÄ±cÄ±nÄ±n ihtiyacÄ±na gÃ¶re detay seviyesini ayarla
- Temel bilgilerle baÅŸla, detaylara in
- KarÅŸÄ±laÅŸtÄ±rmalÄ± analiz yap
- BaÄŸlam iÃ§inde aÃ§Ä±kla

âŒ **YAPMA:**
- âŒ KaynaksÄ±z bilgi verme
- âŒ Yorum ve kiÅŸisel gÃ¶rÃ¼ÅŸ katma
"""

        # Sohbet geÃ§miÅŸini baÅŸlat
        self.messages = []

        # KullanÄ±cÄ±ya ait ek veriler
        self.user_data = {
            "calculations": [],
            "notes": [],
        }

        # Maksimum tutulacak mesaj sayÄ±sÄ± (kayan pencere)
        self.MAX_HISTORY = 15

        # Gemini modelini baÅŸlat
        self.model = genai.GenerativeModel('models/gemini-2.5-flash')

    # Fonksiyon tanÄ±mlarÄ±nÄ± dÃ¶ndÃ¼rÃ¼r (hesaplama ve arama)
    def get_tools(self):
        return [
            {
                "function_declarations": [calculator.get_function_def()]
            },
            {
                "function_declarations": [search.get_function_def()]
            }
        ]

    # Sohbet geÃ§miÅŸini sÄ±fÄ±rlar (kullanÄ±cÄ± "geÃ§miÅŸi temizle" dediÄŸinde kullanÄ±labilir)
    def reset_history(self):
        self.messages = []

    # Mesaj geÃ§miÅŸini kÄ±saltÄ±r
    def _get_limited_history(self):
        return self.messages[-self.MAX_HISTORY:]

    # Harf harf streaming iÃ§in yardÄ±mcÄ± fonksiyon
    def _stream_text_char_by_char(self, text, chunk_size=3):
        """Metni kÃ¼Ã§Ã¼k parÃ§alar halinde yield eder"""
        for i in range(0, len(text), chunk_size):
            yield {"type": "content", "content": text[i:i+chunk_size]}
            time.sleep(0.01)  # Daha akÄ±cÄ± gÃ¶rÃ¼nmesi iÃ§in kÃ¼Ã§Ã¼k delay

    # KullanÄ±cÄ± mesajÄ±nÄ± iÅŸler ve modeli stream halinde Ã§aÄŸÄ±rÄ±r
    def chat_stream(self, user_message):
        try:
            # KullanÄ±cÄ± mesajÄ±nÄ± geÃ§miÅŸe ekle
            self.messages.append({"role": "user", "parts": [{"text": user_message}]})
            print(f"ðŸ“ KullanÄ±cÄ± mesajÄ±: {user_message}")

            # Gemini'yi Ã§aÄŸÄ±r
            chat = self.model.start_chat(history=self._get_limited_history())
            
            # Tools/fonksiyonlarÄ± ekle
            response = chat.send_message(
                user_message,
                tools=self.get_tools(),
                stream=True
            )

            full_content = ""
            function_calls = []
            accumulated_text = ""

            # Response'u stream et - karakter karakter iÅŸle
            for chunk in response:
                if chunk.candidates and chunk.candidates[0].content:
                    # Text content kontrolÃ¼
                    if hasattr(chunk.candidates[0].content.parts[0], 'text'):
                        content = chunk.candidates[0].content.parts[0].text
                        if content:
                            full_content += content
                            accumulated_text += content
                            
                            # KÃ¼Ã§Ã¼k parÃ§alar halinde gÃ¶nder
                            if len(accumulated_text) >= 10:  # 10 karakterde bir gÃ¶nder
                                for stream_chunk in self._stream_text_char_by_char(accumulated_text):
                                    yield stream_chunk
                                accumulated_text = ""
                
                # Fonksiyon Ã§aÄŸrÄ±larÄ±nÄ± kontrol et
                if (chunk.candidates and chunk.candidates[0].content and 
                    chunk.candidates[0].content.parts and 
                    hasattr(chunk.candidates[0].content.parts[0], 'function_call')):
                    
                    function_call = chunk.candidates[0].content.parts[0].function_call
                    fn_name = function_call.name
                    
                    # Args None kontrolÃ¼
                    if hasattr(function_call, 'args') and function_call.args:
                        args = {k: v for k, v in function_call.args.items()}
                    else:
                        args = {}
                    
                    function_calls.append((fn_name, args))
                    yield {"type": "function_call", "function": fn_name, "args": args}

            # Kalan text'i gÃ¶nder
            if accumulated_text:
                for stream_chunk in self._stream_text_char_by_char(accumulated_text):
                    yield stream_chunk

            # Fonksiyon Ã§aÄŸrÄ±larÄ±nÄ± iÅŸle
            if function_calls:
                for fn_name, args in function_calls:
                    # Fonksiyon ismi boÅŸsa veya None'sa atla
                    if not fn_name:
                        print("âš ï¸ BoÅŸ fonksiyon ismi, atlanÄ±yor...")
                        continue
            
                    print(f"ðŸ”§ Fonksiyon Ã§aÄŸrÄ±sÄ±: {fn_name} - {args}")
                    
                    # FonksiyonlarÄ± Ã§alÄ±ÅŸtÄ±r
                    try:
                        if fn_name == "search_info":
                            result = search.search_info(**args)
                        elif fn_name == "calculate":
                            result = calculator.calculate(**args, user_data=self.user_data)
                        else:
                            result = {"error": f"Bilinmeyen fonksiyon: {fn_name}"}
                            yield {"type": "function_result", "result": result}
                            continue
                
                        yield {"type": "function_result", "result": result}
                    
                    except Exception as func_error:
                        result = {"error": f"Fonksiyon hatasÄ±: {str(func_error)}"}
                        yield {"type": "function_result", "result": result}
                        continue

                    # Fonksiyon sonucunu geÃ§miÅŸe ekle
                    self.messages.append({
                        "role": "function",
                        "parts": [{
                            "function_response": {
                                "name": fn_name,
                                "response": result
                            }
                        }]
                    })

                    # Fonksiyon sonucu ile tekrar Ã§aÄŸÄ±r
                    follow_up_response = chat.send_message(
                        f"Fonksiyon sonucu: {result}",
                        stream=True
                    )

                    # Follow-up response'u stream et
                    follow_up_content = ""
                    follow_up_accumulated = ""
                    
                    for follow_chunk in follow_up_response:
                        if follow_chunk.candidates and follow_chunk.candidates[0].content:
                            if hasattr(follow_chunk.candidates[0].content.parts[0], 'text'):
                                content = follow_chunk.candidates[0].content.parts[0].text
                                if content:
                                    follow_up_content += content
                                    follow_up_accumulated += content
                                    
                                    # KÃ¼Ã§Ã¼k parÃ§alar halinde gÃ¶nder
                                    if len(follow_up_accumulated) >= 10:
                                        for stream_chunk in self._stream_text_char_by_char(follow_up_accumulated):
                                            yield stream_chunk
                                        follow_up_accumulated = ""
                    
                    # Kalan follow-up text'i gÃ¶nder
                    if follow_up_accumulated:
                        for stream_chunk in self._stream_text_char_by_char(follow_up_accumulated):
                            yield stream_chunk

                    # Asistan cevabÄ±nÄ± geÃ§miÅŸe ekle
                    if follow_up_content:
                        self.messages.append({"role": "model", "parts": [{"text": follow_up_content}]})
            else:
                # Normal cevabÄ± geÃ§miÅŸe ekle (fonksiyon Ã§aÄŸrÄ±sÄ± yoksa)
                if full_content:
                    self.messages.append({"role": "model", "parts": [{"text": full_content}]})

            # Stream sonu
            yield {"type": "end"}

        except Exception as e:
            print("ðŸ”¥ chat_stream hatasÄ±:", traceback.format_exc())
            yield {"type": "error", "error": str(e), "trace": traceback.format_exc()}